{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMb/MlRn4d+R6KiPTGL/3zC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RenataRSBriet/ciencia-de-dados-e-sistemas-inteligentes/blob/master/RedesNeurais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IS0xAaAwGb1p"
      },
      "outputs": [],
      "source": [
        "import  pickle\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "caminho = \"/content/drive/MyDrive/Aula/MaterialApoio/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ScotYAuIM7F",
        "outputId": "409beac7-764f-4f9c-9247-fecf413851b1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(caminho+'credit.pkl', 'rb') as f:\n",
        "  x_credit_treinamento, y_credit_treinamento, x_credit_teste, y_credit_teste = pickle.load(f)"
      ],
      "metadata": {
        "id": "1L9noqVvYeSP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_credit_treinamento.shape, y_credit_treinamento.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-goLyiTYvv1",
        "outputId": "ce5c92ce-b82c-4c83-e634-25f389727a6b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1500, 3), (1500,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_credit_teste.shape, y_credit_teste.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz-EqybHY871",
        "outputId": "46edb412-b321-468e-9c4d-41391306827a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 3), (500,))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,\n",
        "solver='adam', activation='relu', hidden_layer_sizes=(10,10))\n",
        "rede_neural_credit.fit(x_credit_treinamento, y_credit_treinamento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QkKQh9wrZAvs",
        "outputId": "cf7b46b2-2ac1-45ff-c761-4d0b36ee9527"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.61783593\n",
            "Iteration 2, loss = 0.58416598\n",
            "Iteration 3, loss = 0.55408643\n",
            "Iteration 4, loss = 0.52700387\n",
            "Iteration 5, loss = 0.50238645\n",
            "Iteration 6, loss = 0.47917948\n",
            "Iteration 7, loss = 0.45749201\n",
            "Iteration 8, loss = 0.43718791\n",
            "Iteration 9, loss = 0.41788108\n",
            "Iteration 10, loss = 0.39997598\n",
            "Iteration 11, loss = 0.38322432\n",
            "Iteration 12, loss = 0.36726061\n",
            "Iteration 13, loss = 0.35212378\n",
            "Iteration 14, loss = 0.33730280\n",
            "Iteration 15, loss = 0.32255946\n",
            "Iteration 16, loss = 0.30832939\n",
            "Iteration 17, loss = 0.29443675\n",
            "Iteration 18, loss = 0.28117356\n",
            "Iteration 19, loss = 0.26822678\n",
            "Iteration 20, loss = 0.25599009\n",
            "Iteration 21, loss = 0.24407330\n",
            "Iteration 22, loss = 0.23259616\n",
            "Iteration 23, loss = 0.22168531\n",
            "Iteration 24, loss = 0.21161637\n",
            "Iteration 25, loss = 0.20246143\n",
            "Iteration 26, loss = 0.19374586\n",
            "Iteration 27, loss = 0.18570232\n",
            "Iteration 28, loss = 0.17815470\n",
            "Iteration 29, loss = 0.17128996\n",
            "Iteration 30, loss = 0.16458924\n",
            "Iteration 31, loss = 0.15853404\n",
            "Iteration 32, loss = 0.15284999\n",
            "Iteration 33, loss = 0.14737215\n",
            "Iteration 34, loss = 0.14242582\n",
            "Iteration 35, loss = 0.13742493\n",
            "Iteration 36, loss = 0.13281504\n",
            "Iteration 37, loss = 0.12849428\n",
            "Iteration 38, loss = 0.12446150\n",
            "Iteration 39, loss = 0.12065974\n",
            "Iteration 40, loss = 0.11695076\n",
            "Iteration 41, loss = 0.11350415\n",
            "Iteration 42, loss = 0.11030251\n",
            "Iteration 43, loss = 0.10722106\n",
            "Iteration 44, loss = 0.10442558\n",
            "Iteration 45, loss = 0.10153987\n",
            "Iteration 46, loss = 0.09896774\n",
            "Iteration 47, loss = 0.09648267\n",
            "Iteration 48, loss = 0.09414127\n",
            "Iteration 49, loss = 0.09180533\n",
            "Iteration 50, loss = 0.08970453\n",
            "Iteration 51, loss = 0.08765934\n",
            "Iteration 52, loss = 0.08571966\n",
            "Iteration 53, loss = 0.08370028\n",
            "Iteration 54, loss = 0.08188719\n",
            "Iteration 55, loss = 0.08018341\n",
            "Iteration 56, loss = 0.07849395\n",
            "Iteration 57, loss = 0.07700096\n",
            "Iteration 58, loss = 0.07548659\n",
            "Iteration 59, loss = 0.07405544\n",
            "Iteration 60, loss = 0.07265455\n",
            "Iteration 61, loss = 0.07141444\n",
            "Iteration 62, loss = 0.07006176\n",
            "Iteration 63, loss = 0.06885670\n",
            "Iteration 64, loss = 0.06770296\n",
            "Iteration 65, loss = 0.06673872\n",
            "Iteration 66, loss = 0.06552371\n",
            "Iteration 67, loss = 0.06445761\n",
            "Iteration 68, loss = 0.06348773\n",
            "Iteration 69, loss = 0.06260235\n",
            "Iteration 70, loss = 0.06149138\n",
            "Iteration 71, loss = 0.06060014\n",
            "Iteration 72, loss = 0.05976439\n",
            "Iteration 73, loss = 0.05878279\n",
            "Iteration 74, loss = 0.05798210\n",
            "Iteration 75, loss = 0.05724536\n",
            "Iteration 76, loss = 0.05648917\n",
            "Iteration 77, loss = 0.05564145\n",
            "Iteration 78, loss = 0.05496429\n",
            "Iteration 79, loss = 0.05420100\n",
            "Iteration 80, loss = 0.05348595\n",
            "Iteration 81, loss = 0.05280100\n",
            "Iteration 82, loss = 0.05207836\n",
            "Iteration 83, loss = 0.05151445\n",
            "Iteration 84, loss = 0.05073485\n",
            "Iteration 85, loss = 0.05012054\n",
            "Iteration 86, loss = 0.04946389\n",
            "Iteration 87, loss = 0.04886397\n",
            "Iteration 88, loss = 0.04832235\n",
            "Iteration 89, loss = 0.04772413\n",
            "Iteration 90, loss = 0.04702264\n",
            "Iteration 91, loss = 0.04648983\n",
            "Iteration 92, loss = 0.04606852\n",
            "Iteration 93, loss = 0.04545219\n",
            "Iteration 94, loss = 0.04516131\n",
            "Iteration 95, loss = 0.04449600\n",
            "Iteration 96, loss = 0.04392515\n",
            "Iteration 97, loss = 0.04336966\n",
            "Iteration 98, loss = 0.04294870\n",
            "Iteration 99, loss = 0.04244928\n",
            "Iteration 100, loss = 0.04197267\n",
            "Iteration 101, loss = 0.04155149\n",
            "Iteration 102, loss = 0.04112867\n",
            "Iteration 103, loss = 0.04071831\n",
            "Iteration 104, loss = 0.04032116\n",
            "Iteration 105, loss = 0.03996764\n",
            "Iteration 106, loss = 0.03946601\n",
            "Iteration 107, loss = 0.03903745\n",
            "Iteration 108, loss = 0.03868898\n",
            "Iteration 109, loss = 0.03832259\n",
            "Iteration 110, loss = 0.03792904\n",
            "Iteration 111, loss = 0.03775029\n",
            "Iteration 112, loss = 0.03731398\n",
            "Iteration 113, loss = 0.03687130\n",
            "Iteration 114, loss = 0.03648449\n",
            "Iteration 115, loss = 0.03619718\n",
            "Iteration 116, loss = 0.03588159\n",
            "Iteration 117, loss = 0.03546811\n",
            "Iteration 118, loss = 0.03526119\n",
            "Iteration 119, loss = 0.03492641\n",
            "Iteration 120, loss = 0.03464376\n",
            "Iteration 121, loss = 0.03440880\n",
            "Iteration 122, loss = 0.03402877\n",
            "Iteration 123, loss = 0.03366825\n",
            "Iteration 124, loss = 0.03341437\n",
            "Iteration 125, loss = 0.03307477\n",
            "Iteration 126, loss = 0.03280916\n",
            "Iteration 127, loss = 0.03251211\n",
            "Iteration 128, loss = 0.03221650\n",
            "Iteration 129, loss = 0.03197541\n",
            "Iteration 130, loss = 0.03172206\n",
            "Iteration 131, loss = 0.03145939\n",
            "Iteration 132, loss = 0.03111905\n",
            "Iteration 133, loss = 0.03087321\n",
            "Iteration 134, loss = 0.03059079\n",
            "Iteration 135, loss = 0.03038204\n",
            "Iteration 136, loss = 0.03021174\n",
            "Iteration 137, loss = 0.02992959\n",
            "Iteration 138, loss = 0.02968703\n",
            "Iteration 139, loss = 0.02941702\n",
            "Iteration 140, loss = 0.02908653\n",
            "Iteration 141, loss = 0.02889456\n",
            "Iteration 142, loss = 0.02873765\n",
            "Iteration 143, loss = 0.02853679\n",
            "Iteration 144, loss = 0.02825525\n",
            "Iteration 145, loss = 0.02801363\n",
            "Iteration 146, loss = 0.02782090\n",
            "Iteration 147, loss = 0.02764721\n",
            "Iteration 148, loss = 0.02740632\n",
            "Iteration 149, loss = 0.02725326\n",
            "Iteration 150, loss = 0.02704225\n",
            "Iteration 151, loss = 0.02684057\n",
            "Iteration 152, loss = 0.02657569\n",
            "Iteration 153, loss = 0.02645423\n",
            "Iteration 154, loss = 0.02624881\n",
            "Iteration 155, loss = 0.02601591\n",
            "Iteration 156, loss = 0.02592311\n",
            "Iteration 157, loss = 0.02564941\n",
            "Iteration 158, loss = 0.02549932\n",
            "Iteration 159, loss = 0.02524925\n",
            "Iteration 160, loss = 0.02521867\n",
            "Iteration 161, loss = 0.02497340\n",
            "Iteration 162, loss = 0.02480264\n",
            "Iteration 163, loss = 0.02462420\n",
            "Iteration 164, loss = 0.02442031\n",
            "Iteration 165, loss = 0.02429910\n",
            "Iteration 166, loss = 0.02423568\n",
            "Iteration 167, loss = 0.02392568\n",
            "Iteration 168, loss = 0.02385007\n",
            "Iteration 169, loss = 0.02359204\n",
            "Iteration 170, loss = 0.02350612\n",
            "Iteration 171, loss = 0.02342524\n",
            "Iteration 172, loss = 0.02319745\n",
            "Iteration 173, loss = 0.02303484\n",
            "Iteration 174, loss = 0.02285204\n",
            "Iteration 175, loss = 0.02272771\n",
            "Iteration 176, loss = 0.02265853\n",
            "Iteration 177, loss = 0.02258321\n",
            "Iteration 178, loss = 0.02228443\n",
            "Iteration 179, loss = 0.02214719\n",
            "Iteration 180, loss = 0.02212071\n",
            "Iteration 181, loss = 0.02188737\n",
            "Iteration 182, loss = 0.02174231\n",
            "Iteration 183, loss = 0.02166592\n",
            "Iteration 184, loss = 0.02150025\n",
            "Iteration 185, loss = 0.02139832\n",
            "Iteration 186, loss = 0.02119336\n",
            "Iteration 187, loss = 0.02111780\n",
            "Iteration 188, loss = 0.02097706\n",
            "Iteration 189, loss = 0.02087663\n",
            "Iteration 190, loss = 0.02073741\n",
            "Iteration 191, loss = 0.02063685\n",
            "Iteration 192, loss = 0.02049743\n",
            "Iteration 193, loss = 0.02047567\n",
            "Iteration 194, loss = 0.02028177\n",
            "Iteration 195, loss = 0.02016332\n",
            "Iteration 196, loss = 0.01999414\n",
            "Iteration 197, loss = 0.01985825\n",
            "Iteration 198, loss = 0.01979807\n",
            "Iteration 199, loss = 0.01967313\n",
            "Iteration 200, loss = 0.01952592\n",
            "Iteration 201, loss = 0.01943793\n",
            "Iteration 202, loss = 0.01934403\n",
            "Iteration 203, loss = 0.01921143\n",
            "Iteration 204, loss = 0.01913291\n",
            "Iteration 205, loss = 0.01897014\n",
            "Iteration 206, loss = 0.01885728\n",
            "Iteration 207, loss = 0.01875987\n",
            "Iteration 208, loss = 0.01867792\n",
            "Iteration 209, loss = 0.01857269\n",
            "Iteration 210, loss = 0.01842572\n",
            "Iteration 211, loss = 0.01833729\n",
            "Iteration 212, loss = 0.01823812\n",
            "Iteration 213, loss = 0.01814704\n",
            "Iteration 214, loss = 0.01812815\n",
            "Iteration 215, loss = 0.01788682\n",
            "Iteration 216, loss = 0.01790898\n",
            "Iteration 217, loss = 0.01789662\n",
            "Iteration 218, loss = 0.01775092\n",
            "Iteration 219, loss = 0.01749248\n",
            "Iteration 220, loss = 0.01743123\n",
            "Iteration 221, loss = 0.01742970\n",
            "Iteration 222, loss = 0.01724386\n",
            "Iteration 223, loss = 0.01722574\n",
            "Iteration 224, loss = 0.01717388\n",
            "Iteration 225, loss = 0.01693091\n",
            "Iteration 226, loss = 0.01698611\n",
            "Iteration 227, loss = 0.01689134\n",
            "Iteration 228, loss = 0.01668473\n",
            "Iteration 229, loss = 0.01657542\n",
            "Iteration 230, loss = 0.01646747\n",
            "Iteration 231, loss = 0.01637287\n",
            "Iteration 232, loss = 0.01625160\n",
            "Iteration 233, loss = 0.01615613\n",
            "Iteration 234, loss = 0.01606667\n",
            "Iteration 235, loss = 0.01604155\n",
            "Iteration 236, loss = 0.01599964\n",
            "Iteration 237, loss = 0.01600310\n",
            "Iteration 238, loss = 0.01575977\n",
            "Iteration 239, loss = 0.01569006\n",
            "Iteration 240, loss = 0.01553380\n",
            "Iteration 241, loss = 0.01545954\n",
            "Iteration 242, loss = 0.01532911\n",
            "Iteration 243, loss = 0.01526817\n",
            "Iteration 244, loss = 0.01519606\n",
            "Iteration 245, loss = 0.01508336\n",
            "Iteration 246, loss = 0.01507004\n",
            "Iteration 247, loss = 0.01500073\n",
            "Iteration 248, loss = 0.01491604\n",
            "Iteration 249, loss = 0.01474331\n",
            "Iteration 250, loss = 0.01470912\n",
            "Iteration 251, loss = 0.01458659\n",
            "Iteration 252, loss = 0.01456780\n",
            "Iteration 253, loss = 0.01452507\n",
            "Iteration 254, loss = 0.01430274\n",
            "Iteration 255, loss = 0.01445906\n",
            "Iteration 256, loss = 0.01439385\n",
            "Iteration 257, loss = 0.01420222\n",
            "Iteration 258, loss = 0.01408999\n",
            "Iteration 259, loss = 0.01397838\n",
            "Iteration 260, loss = 0.01405588\n",
            "Iteration 261, loss = 0.01404450\n",
            "Iteration 262, loss = 0.01378672\n",
            "Iteration 263, loss = 0.01373893\n",
            "Iteration 264, loss = 0.01365197\n",
            "Iteration 265, loss = 0.01361837\n",
            "Iteration 266, loss = 0.01353526\n",
            "Iteration 267, loss = 0.01356534\n",
            "Iteration 268, loss = 0.01337623\n",
            "Iteration 269, loss = 0.01335293\n",
            "Iteration 270, loss = 0.01335288\n",
            "Iteration 271, loss = 0.01317853\n",
            "Iteration 272, loss = 0.01308768\n",
            "Iteration 273, loss = 0.01303963\n",
            "Iteration 274, loss = 0.01296273\n",
            "Iteration 275, loss = 0.01289141\n",
            "Iteration 276, loss = 0.01278032\n",
            "Iteration 277, loss = 0.01280784\n",
            "Iteration 278, loss = 0.01275317\n",
            "Iteration 279, loss = 0.01262496\n",
            "Iteration 280, loss = 0.01256320\n",
            "Iteration 281, loss = 0.01252824\n",
            "Iteration 282, loss = 0.01247534\n",
            "Iteration 283, loss = 0.01244207\n",
            "Iteration 284, loss = 0.01238245\n",
            "Iteration 285, loss = 0.01228918\n",
            "Iteration 286, loss = 0.01223402\n",
            "Iteration 287, loss = 0.01218490\n",
            "Iteration 288, loss = 0.01215794\n",
            "Iteration 289, loss = 0.01210822\n",
            "Iteration 290, loss = 0.01205446\n",
            "Iteration 291, loss = 0.01197839\n",
            "Iteration 292, loss = 0.01204169\n",
            "Iteration 293, loss = 0.01196219\n",
            "Iteration 294, loss = 0.01184625\n",
            "Iteration 295, loss = 0.01192645\n",
            "Iteration 296, loss = 0.01176965\n",
            "Iteration 297, loss = 0.01171820\n",
            "Iteration 298, loss = 0.01162941\n",
            "Iteration 299, loss = 0.01163248\n",
            "Iteration 300, loss = 0.01159981\n",
            "Iteration 301, loss = 0.01151588\n",
            "Iteration 302, loss = 0.01144452\n",
            "Iteration 303, loss = 0.01141791\n",
            "Iteration 304, loss = 0.01135897\n",
            "Iteration 305, loss = 0.01131206\n",
            "Iteration 306, loss = 0.01122138\n",
            "Iteration 307, loss = 0.01120382\n",
            "Iteration 308, loss = 0.01122300\n",
            "Iteration 309, loss = 0.01114083\n",
            "Iteration 310, loss = 0.01114157\n",
            "Iteration 311, loss = 0.01109154\n",
            "Iteration 312, loss = 0.01100909\n",
            "Iteration 313, loss = 0.01093807\n",
            "Iteration 314, loss = 0.01110330\n",
            "Iteration 315, loss = 0.01085529\n",
            "Iteration 316, loss = 0.01080799\n",
            "Iteration 317, loss = 0.01085109\n",
            "Iteration 318, loss = 0.01075398\n",
            "Iteration 319, loss = 0.01071443\n",
            "Iteration 320, loss = 0.01090371\n",
            "Iteration 321, loss = 0.01058957\n",
            "Iteration 322, loss = 0.01064342\n",
            "Iteration 323, loss = 0.01051772\n",
            "Iteration 324, loss = 0.01041376\n",
            "Iteration 325, loss = 0.01048723\n",
            "Iteration 326, loss = 0.01042304\n",
            "Iteration 327, loss = 0.01034062\n",
            "Iteration 328, loss = 0.01047770\n",
            "Iteration 329, loss = 0.01022130\n",
            "Iteration 330, loss = 0.01039675\n",
            "Iteration 331, loss = 0.01032243\n",
            "Iteration 332, loss = 0.01017170\n",
            "Iteration 333, loss = 0.01027080\n",
            "Iteration 334, loss = 0.01018144\n",
            "Iteration 335, loss = 0.01005896\n",
            "Iteration 336, loss = 0.01002962\n",
            "Iteration 337, loss = 0.00997768\n",
            "Iteration 338, loss = 0.00988701\n",
            "Iteration 339, loss = 0.00992020\n",
            "Iteration 340, loss = 0.00990685\n",
            "Iteration 341, loss = 0.00980512\n",
            "Iteration 342, loss = 0.00976733\n",
            "Iteration 343, loss = 0.00982725\n",
            "Iteration 344, loss = 0.00968316\n",
            "Iteration 345, loss = 0.00979401\n",
            "Iteration 346, loss = 0.00975591\n",
            "Iteration 347, loss = 0.00964763\n",
            "Iteration 348, loss = 0.00955576\n",
            "Iteration 349, loss = 0.00949428\n",
            "Iteration 350, loss = 0.00948809\n",
            "Iteration 351, loss = 0.00940937\n",
            "Iteration 352, loss = 0.00938352\n",
            "Iteration 353, loss = 0.00941838\n",
            "Iteration 354, loss = 0.00931437\n",
            "Iteration 355, loss = 0.00926808\n",
            "Iteration 356, loss = 0.00933270\n",
            "Iteration 357, loss = 0.00925494\n",
            "Iteration 358, loss = 0.00934180\n",
            "Iteration 359, loss = 0.00926532\n",
            "Iteration 360, loss = 0.00907941\n",
            "Iteration 361, loss = 0.00908610\n",
            "Iteration 362, loss = 0.00907752\n",
            "Iteration 363, loss = 0.00903002\n",
            "Iteration 364, loss = 0.00896171\n",
            "Iteration 365, loss = 0.00896121\n",
            "Iteration 366, loss = 0.00891516\n",
            "Iteration 367, loss = 0.00892666\n",
            "Iteration 368, loss = 0.00882463\n",
            "Iteration 369, loss = 0.00881616\n",
            "Iteration 370, loss = 0.00875840\n",
            "Iteration 371, loss = 0.00877375\n",
            "Iteration 372, loss = 0.00883030\n",
            "Iteration 373, loss = 0.00873208\n",
            "Iteration 374, loss = 0.00882727\n",
            "Iteration 375, loss = 0.00864679\n",
            "Iteration 376, loss = 0.00859133\n",
            "Iteration 377, loss = 0.00857925\n",
            "Iteration 378, loss = 0.00851754\n",
            "Iteration 379, loss = 0.00847947\n",
            "Iteration 380, loss = 0.00846612\n",
            "Iteration 381, loss = 0.00844947\n",
            "Iteration 382, loss = 0.00842112\n",
            "Iteration 383, loss = 0.00838891\n",
            "Iteration 384, loss = 0.00840306\n",
            "Iteration 385, loss = 0.00832568\n",
            "Iteration 386, loss = 0.00833520\n",
            "Iteration 387, loss = 0.00828291\n",
            "Iteration 388, loss = 0.00824223\n",
            "Iteration 389, loss = 0.00826474\n",
            "Iteration 390, loss = 0.00828973\n",
            "Iteration 391, loss = 0.00815845\n",
            "Iteration 392, loss = 0.00823784\n",
            "Iteration 393, loss = 0.00815126\n",
            "Iteration 394, loss = 0.00807545\n",
            "Iteration 395, loss = 0.00801666\n",
            "Iteration 396, loss = 0.00804354\n",
            "Iteration 397, loss = 0.00804130\n",
            "Iteration 398, loss = 0.00799155\n",
            "Iteration 399, loss = 0.00807730\n",
            "Iteration 400, loss = 0.00792755\n",
            "Iteration 401, loss = 0.00792866\n",
            "Iteration 402, loss = 0.00789469\n",
            "Iteration 403, loss = 0.00781254\n",
            "Iteration 404, loss = 0.00777579\n",
            "Iteration 405, loss = 0.00781226\n",
            "Iteration 406, loss = 0.00774588\n",
            "Iteration 407, loss = 0.00779437\n",
            "Iteration 408, loss = 0.00771275\n",
            "Iteration 409, loss = 0.00770620\n",
            "Iteration 410, loss = 0.00768155\n",
            "Iteration 411, loss = 0.00765261\n",
            "Iteration 412, loss = 0.00758664\n",
            "Iteration 413, loss = 0.00758009\n",
            "Iteration 414, loss = 0.00761574\n",
            "Iteration 415, loss = 0.00756354\n",
            "Iteration 416, loss = 0.00748912\n",
            "Iteration 417, loss = 0.00746286\n",
            "Iteration 418, loss = 0.00751569\n",
            "Iteration 419, loss = 0.00744413\n",
            "Iteration 420, loss = 0.00736109\n",
            "Iteration 421, loss = 0.00742890\n",
            "Iteration 422, loss = 0.00741167\n",
            "Iteration 423, loss = 0.00739825\n",
            "Iteration 424, loss = 0.00731122\n",
            "Iteration 425, loss = 0.00734451\n",
            "Iteration 426, loss = 0.00724786\n",
            "Iteration 427, loss = 0.00721032\n",
            "Iteration 428, loss = 0.00722188\n",
            "Iteration 429, loss = 0.00712409\n",
            "Iteration 430, loss = 0.00718823\n",
            "Iteration 431, loss = 0.00712209\n",
            "Iteration 432, loss = 0.00710526\n",
            "Iteration 433, loss = 0.00706970\n",
            "Iteration 434, loss = 0.00701236\n",
            "Iteration 435, loss = 0.00704576\n",
            "Iteration 436, loss = 0.00697604\n",
            "Iteration 437, loss = 0.00699011\n",
            "Iteration 438, loss = 0.00704007\n",
            "Iteration 439, loss = 0.00690151\n",
            "Iteration 440, loss = 0.00693575\n",
            "Iteration 441, loss = 0.00700067\n",
            "Iteration 442, loss = 0.00694264\n",
            "Iteration 443, loss = 0.00685433\n",
            "Iteration 444, loss = 0.00678863\n",
            "Iteration 445, loss = 0.00678904\n",
            "Iteration 446, loss = 0.00675578\n",
            "Iteration 447, loss = 0.00679953\n",
            "Iteration 448, loss = 0.00734786\n",
            "Iteration 449, loss = 0.00675248\n",
            "Iteration 450, loss = 0.00685396\n",
            "Iteration 451, loss = 0.00669939\n",
            "Iteration 452, loss = 0.00661027\n",
            "Iteration 453, loss = 0.00666624\n",
            "Iteration 454, loss = 0.00669428\n",
            "Iteration 455, loss = 0.00670558\n",
            "Iteration 456, loss = 0.00649783\n",
            "Iteration 457, loss = 0.00651532\n",
            "Iteration 458, loss = 0.00656062\n",
            "Iteration 459, loss = 0.00654806\n",
            "Iteration 460, loss = 0.00646718\n",
            "Iteration 461, loss = 0.00646716\n",
            "Iteration 462, loss = 0.00653063\n",
            "Iteration 463, loss = 0.00641157\n",
            "Iteration 464, loss = 0.00645181\n",
            "Iteration 465, loss = 0.00632030\n",
            "Iteration 466, loss = 0.00633025\n",
            "Iteration 467, loss = 0.00632208\n",
            "Iteration 468, loss = 0.00638321\n",
            "Iteration 469, loss = 0.00636001\n",
            "Iteration 470, loss = 0.00641347\n",
            "Iteration 471, loss = 0.00631211\n",
            "Iteration 472, loss = 0.00629833\n",
            "Iteration 473, loss = 0.00632071\n",
            "Iteration 474, loss = 0.00623719\n",
            "Iteration 475, loss = 0.00621390\n",
            "Iteration 476, loss = 0.00623184\n",
            "Iteration 477, loss = 0.00628399\n",
            "Iteration 478, loss = 0.00614467\n",
            "Iteration 479, loss = 0.00612880\n",
            "Iteration 480, loss = 0.00608011\n",
            "Iteration 481, loss = 0.00614713\n",
            "Iteration 482, loss = 0.00609342\n",
            "Iteration 483, loss = 0.00607506\n",
            "Iteration 484, loss = 0.00602542\n",
            "Iteration 485, loss = 0.00599431\n",
            "Iteration 486, loss = 0.00597120\n",
            "Iteration 487, loss = 0.00593561\n",
            "Iteration 488, loss = 0.00592379\n",
            "Iteration 489, loss = 0.00593243\n",
            "Iteration 490, loss = 0.00588707\n",
            "Iteration 491, loss = 0.00593146\n",
            "Iteration 492, loss = 0.00583348\n",
            "Iteration 493, loss = 0.00589093\n",
            "Iteration 494, loss = 0.00580074\n",
            "Iteration 495, loss = 0.00581335\n",
            "Iteration 496, loss = 0.00581978\n",
            "Iteration 497, loss = 0.00577307\n",
            "Iteration 498, loss = 0.00571969\n",
            "Iteration 499, loss = 0.00572377\n",
            "Iteration 500, loss = 0.00571385\n",
            "Iteration 501, loss = 0.00566731\n",
            "Iteration 502, loss = 0.00570505\n",
            "Iteration 503, loss = 0.00569808\n",
            "Iteration 504, loss = 0.00567143\n",
            "Iteration 505, loss = 0.00564059\n",
            "Iteration 506, loss = 0.00563145\n",
            "Iteration 507, loss = 0.00568849\n",
            "Iteration 508, loss = 0.00562881\n",
            "Iteration 509, loss = 0.00553528\n",
            "Iteration 510, loss = 0.00553368\n",
            "Iteration 511, loss = 0.00556393\n",
            "Iteration 512, loss = 0.00547981\n",
            "Iteration 513, loss = 0.00569584\n",
            "Iteration 514, loss = 0.00556722\n",
            "Iteration 515, loss = 0.00550277\n",
            "Iteration 516, loss = 0.00548642\n",
            "Iteration 517, loss = 0.00541881\n",
            "Iteration 518, loss = 0.00538932\n",
            "Iteration 519, loss = 0.00535855\n",
            "Iteration 520, loss = 0.00534461\n",
            "Iteration 521, loss = 0.00541781\n",
            "Iteration 522, loss = 0.00538587\n",
            "Iteration 523, loss = 0.00529725\n",
            "Iteration 524, loss = 0.00532668\n",
            "Iteration 525, loss = 0.00533377\n",
            "Iteration 526, loss = 0.00536869\n",
            "Iteration 527, loss = 0.00532951\n",
            "Iteration 528, loss = 0.00523364\n",
            "Iteration 529, loss = 0.00530997\n",
            "Iteration 530, loss = 0.00525312\n",
            "Iteration 531, loss = 0.00524387\n",
            "Iteration 532, loss = 0.00520988\n",
            "Iteration 533, loss = 0.00514428\n",
            "Iteration 534, loss = 0.00519860\n",
            "Iteration 535, loss = 0.00515432\n",
            "Iteration 536, loss = 0.00517315\n",
            "Iteration 537, loss = 0.00509972\n",
            "Iteration 538, loss = 0.00515185\n",
            "Iteration 539, loss = 0.00506774\n",
            "Iteration 540, loss = 0.00511758\n",
            "Iteration 541, loss = 0.00512074\n",
            "Iteration 542, loss = 0.00509580\n",
            "Iteration 543, loss = 0.00504595\n",
            "Iteration 544, loss = 0.00503133\n",
            "Iteration 545, loss = 0.00496130\n",
            "Iteration 546, loss = 0.00502845\n",
            "Iteration 547, loss = 0.00507431\n",
            "Iteration 548, loss = 0.00497429\n",
            "Iteration 549, loss = 0.00491779\n",
            "Iteration 550, loss = 0.00494478\n",
            "Iteration 551, loss = 0.00495565\n",
            "Iteration 552, loss = 0.00493834\n",
            "Iteration 553, loss = 0.00496987\n",
            "Iteration 554, loss = 0.00482451\n",
            "Iteration 555, loss = 0.00486249\n",
            "Iteration 556, loss = 0.00483731\n",
            "Iteration 557, loss = 0.00485030\n",
            "Iteration 558, loss = 0.00481397\n",
            "Iteration 559, loss = 0.00491642\n",
            "Iteration 560, loss = 0.00480216\n",
            "Iteration 561, loss = 0.00479500\n",
            "Iteration 562, loss = 0.00478001\n",
            "Iteration 563, loss = 0.00474565\n",
            "Iteration 564, loss = 0.00471026\n",
            "Iteration 565, loss = 0.00471491\n",
            "Iteration 566, loss = 0.00470495\n",
            "Iteration 567, loss = 0.00470339\n",
            "Iteration 568, loss = 0.00471720\n",
            "Iteration 569, loss = 0.00464843\n",
            "Iteration 570, loss = 0.00465448\n",
            "Iteration 571, loss = 0.00461234\n",
            "Iteration 572, loss = 0.00459447\n",
            "Iteration 573, loss = 0.00465344\n",
            "Iteration 574, loss = 0.00456792\n",
            "Iteration 575, loss = 0.00458707\n",
            "Iteration 576, loss = 0.00456640\n",
            "Iteration 577, loss = 0.00462332\n",
            "Iteration 578, loss = 0.00457194\n",
            "Iteration 579, loss = 0.00452267\n",
            "Iteration 580, loss = 0.00460633\n",
            "Iteration 581, loss = 0.00445602\n",
            "Iteration 582, loss = 0.00449280\n",
            "Iteration 583, loss = 0.00451254\n",
            "Iteration 584, loss = 0.00448908\n",
            "Iteration 585, loss = 0.00439910\n",
            "Iteration 586, loss = 0.00451862\n",
            "Iteration 587, loss = 0.00448504\n",
            "Iteration 588, loss = 0.00446829\n",
            "Iteration 589, loss = 0.00443093\n",
            "Iteration 590, loss = 0.00435582\n",
            "Iteration 591, loss = 0.00441477\n",
            "Iteration 592, loss = 0.00446417\n",
            "Iteration 593, loss = 0.00442651\n",
            "Iteration 594, loss = 0.00434371\n",
            "Iteration 595, loss = 0.00429625\n",
            "Iteration 596, loss = 0.00436514\n",
            "Iteration 597, loss = 0.00435956\n",
            "Iteration 598, loss = 0.00432633\n",
            "Iteration 599, loss = 0.00432440\n",
            "Iteration 600, loss = 0.00429082\n",
            "Iteration 601, loss = 0.00431812\n",
            "Iteration 602, loss = 0.00430428\n",
            "Iteration 603, loss = 0.00423444\n",
            "Iteration 604, loss = 0.00421434\n",
            "Iteration 605, loss = 0.00424354\n",
            "Iteration 606, loss = 0.00422604\n",
            "Iteration 607, loss = 0.00426712\n",
            "Iteration 608, loss = 0.00420304\n",
            "Iteration 609, loss = 0.00416531\n",
            "Iteration 610, loss = 0.00426839\n",
            "Iteration 611, loss = 0.00423997\n",
            "Iteration 612, loss = 0.00422006\n",
            "Iteration 613, loss = 0.00410185\n",
            "Iteration 614, loss = 0.00420067\n",
            "Iteration 615, loss = 0.00420367\n",
            "Iteration 616, loss = 0.00405759\n",
            "Iteration 617, loss = 0.00409490\n",
            "Iteration 618, loss = 0.00411041\n",
            "Iteration 619, loss = 0.00428480\n",
            "Iteration 620, loss = 0.00409582\n",
            "Iteration 621, loss = 0.00405066\n",
            "Iteration 622, loss = 0.00404017\n",
            "Iteration 623, loss = 0.00400881\n",
            "Iteration 624, loss = 0.00400381\n",
            "Iteration 625, loss = 0.00395485\n",
            "Iteration 626, loss = 0.00396065\n",
            "Iteration 627, loss = 0.00400415\n",
            "Iteration 628, loss = 0.00392430\n",
            "Iteration 629, loss = 0.00388410\n",
            "Iteration 630, loss = 0.00397001\n",
            "Iteration 631, loss = 0.00396054\n",
            "Iteration 632, loss = 0.00395122\n",
            "Iteration 633, loss = 0.00388690\n",
            "Iteration 634, loss = 0.00391825\n",
            "Iteration 635, loss = 0.00391701\n",
            "Iteration 636, loss = 0.00386752\n",
            "Iteration 637, loss = 0.00389844\n",
            "Iteration 638, loss = 0.00396270\n",
            "Iteration 639, loss = 0.00395936\n",
            "Iteration 640, loss = 0.00382523\n",
            "Iteration 641, loss = 0.00380732\n",
            "Iteration 642, loss = 0.00379635\n",
            "Iteration 643, loss = 0.00379289\n",
            "Iteration 644, loss = 0.00379374\n",
            "Iteration 645, loss = 0.00377167\n",
            "Iteration 646, loss = 0.00376470\n",
            "Iteration 647, loss = 0.00373851\n",
            "Iteration 648, loss = 0.00371637\n",
            "Iteration 649, loss = 0.00372684\n",
            "Iteration 650, loss = 0.00368857\n",
            "Iteration 651, loss = 0.00370125\n",
            "Iteration 652, loss = 0.00382423\n",
            "Iteration 653, loss = 0.00373477\n",
            "Iteration 654, loss = 0.00368978\n",
            "Iteration 655, loss = 0.00366433\n",
            "Iteration 656, loss = 0.00369250\n",
            "Iteration 657, loss = 0.00360954\n",
            "Iteration 658, loss = 0.00364518\n",
            "Iteration 659, loss = 0.00362846\n",
            "Iteration 660, loss = 0.00363176\n",
            "Iteration 661, loss = 0.00363072\n",
            "Iteration 662, loss = 0.00370829\n",
            "Iteration 663, loss = 0.00369029\n",
            "Iteration 664, loss = 0.00362406\n",
            "Iteration 665, loss = 0.00363936\n",
            "Iteration 666, loss = 0.00356206\n",
            "Iteration 667, loss = 0.00361476\n",
            "Iteration 668, loss = 0.00356097\n",
            "Iteration 669, loss = 0.00360825\n",
            "Iteration 670, loss = 0.00349394\n",
            "Iteration 671, loss = 0.00353268\n",
            "Iteration 672, loss = 0.00359314\n",
            "Iteration 673, loss = 0.00356725\n",
            "Iteration 674, loss = 0.00360060\n",
            "Iteration 675, loss = 0.00347010\n",
            "Iteration 676, loss = 0.00348529\n",
            "Iteration 677, loss = 0.00350439\n",
            "Iteration 678, loss = 0.00345242\n",
            "Iteration 679, loss = 0.00341529\n",
            "Iteration 680, loss = 0.00348617\n",
            "Iteration 681, loss = 0.00348367\n",
            "Iteration 682, loss = 0.00355730\n",
            "Iteration 683, loss = 0.00353761\n",
            "Iteration 684, loss = 0.00340750\n",
            "Iteration 685, loss = 0.00338818\n",
            "Iteration 686, loss = 0.00337060\n",
            "Iteration 687, loss = 0.00334073\n",
            "Iteration 688, loss = 0.00339689\n",
            "Iteration 689, loss = 0.00340244\n",
            "Iteration 690, loss = 0.00340548\n",
            "Iteration 691, loss = 0.00337928\n",
            "Iteration 692, loss = 0.00329543\n",
            "Iteration 693, loss = 0.00338640\n",
            "Iteration 694, loss = 0.00328823\n",
            "Iteration 695, loss = 0.00333598\n",
            "Iteration 696, loss = 0.00336855\n",
            "Iteration 697, loss = 0.00329730\n",
            "Iteration 698, loss = 0.00328427\n",
            "Iteration 699, loss = 0.00327440\n",
            "Iteration 700, loss = 0.00326361\n",
            "Iteration 701, loss = 0.00334208\n",
            "Iteration 702, loss = 0.00331171\n",
            "Iteration 703, loss = 0.00322143\n",
            "Iteration 704, loss = 0.00326828\n",
            "Iteration 705, loss = 0.00325788\n",
            "Iteration 706, loss = 0.00332215\n",
            "Iteration 707, loss = 0.00323500\n",
            "Iteration 708, loss = 0.00322592\n",
            "Iteration 709, loss = 0.00316889\n",
            "Iteration 710, loss = 0.00315804\n",
            "Iteration 711, loss = 0.00314443\n",
            "Iteration 712, loss = 0.00322844\n",
            "Iteration 713, loss = 0.00315990\n",
            "Iteration 714, loss = 0.00316506\n",
            "Iteration 715, loss = 0.00313506\n",
            "Iteration 716, loss = 0.00314935\n",
            "Iteration 717, loss = 0.00319909\n",
            "Iteration 718, loss = 0.00313705\n",
            "Iteration 719, loss = 0.00316333\n",
            "Iteration 720, loss = 0.00324190\n",
            "Iteration 721, loss = 0.00321056\n",
            "Iteration 722, loss = 0.00306934\n",
            "Iteration 723, loss = 0.00305381\n",
            "Iteration 724, loss = 0.00309005\n",
            "Iteration 725, loss = 0.00306277\n",
            "Iteration 726, loss = 0.00304365\n",
            "Iteration 727, loss = 0.00308876\n",
            "Iteration 728, loss = 0.00301495\n",
            "Iteration 729, loss = 0.00296207\n",
            "Iteration 730, loss = 0.00301132\n",
            "Iteration 731, loss = 0.00305355\n",
            "Iteration 732, loss = 0.00297695\n",
            "Iteration 733, loss = 0.00305315\n",
            "Iteration 734, loss = 0.00301344\n",
            "Iteration 735, loss = 0.00299673\n",
            "Iteration 736, loss = 0.00298356\n",
            "Iteration 737, loss = 0.00293252\n",
            "Iteration 738, loss = 0.00295766\n",
            "Iteration 739, loss = 0.00295531\n",
            "Iteration 740, loss = 0.00294509\n",
            "Iteration 741, loss = 0.00292485\n",
            "Iteration 742, loss = 0.00291494\n",
            "Iteration 743, loss = 0.00293248\n",
            "Iteration 744, loss = 0.00289687\n",
            "Iteration 745, loss = 0.00289141\n",
            "Iteration 746, loss = 0.00297837\n",
            "Iteration 747, loss = 0.00291525\n",
            "Iteration 748, loss = 0.00290454\n",
            "Iteration 749, loss = 0.00289963\n",
            "Iteration 750, loss = 0.00291053\n",
            "Iteration 751, loss = 0.00285673\n",
            "Iteration 752, loss = 0.00292913\n",
            "Iteration 753, loss = 0.00279780\n",
            "Iteration 754, loss = 0.00291489\n",
            "Iteration 755, loss = 0.00284540\n",
            "Iteration 756, loss = 0.00288244\n",
            "Iteration 757, loss = 0.00286471\n",
            "Iteration 758, loss = 0.00284633\n",
            "Iteration 759, loss = 0.00287277\n",
            "Iteration 760, loss = 0.00281621\n",
            "Iteration 761, loss = 0.00281179\n",
            "Iteration 762, loss = 0.00278623\n",
            "Iteration 763, loss = 0.00280237\n",
            "Iteration 764, loss = 0.00272979\n",
            "Iteration 765, loss = 0.00277637\n",
            "Iteration 766, loss = 0.00277736\n",
            "Iteration 767, loss = 0.00274558\n",
            "Iteration 768, loss = 0.00275064\n",
            "Iteration 769, loss = 0.00275023\n",
            "Iteration 770, loss = 0.00274304\n",
            "Iteration 771, loss = 0.00275557\n",
            "Iteration 772, loss = 0.00269748\n",
            "Iteration 773, loss = 0.00274529\n",
            "Iteration 774, loss = 0.00271711\n",
            "Iteration 775, loss = 0.00270255\n",
            "Iteration 776, loss = 0.00274959\n",
            "Iteration 777, loss = 0.00278537\n",
            "Iteration 778, loss = 0.00268513\n",
            "Iteration 779, loss = 0.00268459\n",
            "Iteration 780, loss = 0.00269641\n",
            "Iteration 781, loss = 0.00266650\n",
            "Iteration 782, loss = 0.00262874\n",
            "Iteration 783, loss = 0.00267680\n",
            "Iteration 784, loss = 0.00264657\n",
            "Iteration 785, loss = 0.00260654\n",
            "Iteration 786, loss = 0.00261957\n",
            "Iteration 787, loss = 0.00273001\n",
            "Iteration 788, loss = 0.00262472\n",
            "Iteration 789, loss = 0.00258481\n",
            "Iteration 790, loss = 0.00263624\n",
            "Iteration 791, loss = 0.00261436\n",
            "Iteration 792, loss = 0.00257259\n",
            "Iteration 793, loss = 0.00255151\n",
            "Iteration 794, loss = 0.00258486\n",
            "Iteration 795, loss = 0.00259363\n",
            "Iteration 796, loss = 0.00268429\n",
            "Iteration 797, loss = 0.00259012\n",
            "Iteration 798, loss = 0.00259332\n",
            "Iteration 799, loss = 0.00252882\n",
            "Iteration 800, loss = 0.00256496\n",
            "Iteration 801, loss = 0.00253273\n",
            "Iteration 802, loss = 0.00253533\n",
            "Iteration 803, loss = 0.00251748\n",
            "Iteration 804, loss = 0.00248172\n",
            "Iteration 805, loss = 0.00253754\n",
            "Iteration 806, loss = 0.00256017\n",
            "Iteration 807, loss = 0.00252265\n",
            "Iteration 808, loss = 0.00257119\n",
            "Iteration 809, loss = 0.00248267\n",
            "Iteration 810, loss = 0.00252860\n",
            "Iteration 811, loss = 0.00251229\n",
            "Iteration 812, loss = 0.00248642\n",
            "Iteration 813, loss = 0.00244727\n",
            "Iteration 814, loss = 0.00247038\n",
            "Iteration 815, loss = 0.00241721\n",
            "Iteration 816, loss = 0.00248585\n",
            "Iteration 817, loss = 0.00249680\n",
            "Iteration 818, loss = 0.00246273\n",
            "Iteration 819, loss = 0.00243574\n",
            "Iteration 820, loss = 0.00244804\n",
            "Iteration 821, loss = 0.00237168\n",
            "Iteration 822, loss = 0.00249298\n",
            "Iteration 823, loss = 0.00238299\n",
            "Iteration 824, loss = 0.00242299\n",
            "Iteration 825, loss = 0.00249197\n",
            "Iteration 826, loss = 0.00252392\n",
            "Iteration 827, loss = 0.00246324\n",
            "Iteration 828, loss = 0.00245110\n",
            "Iteration 829, loss = 0.00246837\n",
            "Iteration 830, loss = 0.00234996\n",
            "Iteration 831, loss = 0.00242878\n",
            "Iteration 832, loss = 0.00248723\n",
            "Iteration 833, loss = 0.00243715\n",
            "Iteration 834, loss = 0.00233654\n",
            "Iteration 835, loss = 0.00237134\n",
            "Iteration 836, loss = 0.00235969\n",
            "Iteration 837, loss = 0.00242313\n",
            "Iteration 838, loss = 0.00235263\n",
            "Iteration 839, loss = 0.00227491\n",
            "Iteration 840, loss = 0.00247091\n",
            "Iteration 841, loss = 0.00248532\n",
            "Iteration 842, loss = 0.00228960\n",
            "Iteration 843, loss = 0.00228375\n",
            "Iteration 844, loss = 0.00228876\n",
            "Iteration 845, loss = 0.00230266\n",
            "Iteration 846, loss = 0.00229716\n",
            "Iteration 847, loss = 0.00226814\n",
            "Iteration 848, loss = 0.00238286\n",
            "Iteration 849, loss = 0.00222664\n",
            "Iteration 850, loss = 0.00223841\n",
            "Iteration 851, loss = 0.00229663\n",
            "Iteration 852, loss = 0.00225391\n",
            "Iteration 853, loss = 0.00221641\n",
            "Iteration 854, loss = 0.00230458\n",
            "Iteration 855, loss = 0.00228310\n",
            "Iteration 856, loss = 0.00224586\n",
            "Iteration 857, loss = 0.00221166\n",
            "Iteration 858, loss = 0.00227486\n",
            "Iteration 859, loss = 0.00219591\n",
            "Iteration 860, loss = 0.00216511\n",
            "Iteration 861, loss = 0.00224332\n",
            "Iteration 862, loss = 0.00221411\n",
            "Iteration 863, loss = 0.00216544\n",
            "Iteration 864, loss = 0.00216825\n",
            "Iteration 865, loss = 0.00216905\n",
            "Iteration 866, loss = 0.00226176\n",
            "Iteration 867, loss = 0.00215974\n",
            "Iteration 868, loss = 0.00229302\n",
            "Iteration 869, loss = 0.00222756\n",
            "Iteration 870, loss = 0.00215444\n",
            "Iteration 871, loss = 0.00222466\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsao_credit = rede_neural_credit.predict(x_credit_teste)\n",
        "previsao_credit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztq6ZgciiQ6i",
        "outputId": "091c7431-6a9a-484b-cef0-e0c930b01f29"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_credit_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUTPLyNihI-T",
        "outputId": "f8762017-ae28-4ddf-8549-2d891221894e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_credit_teste, previsao_credit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHb5IIWuhKl8",
        "outputId": "2b36dd2e-6ad7-4ac3-dbfa-361701900cc2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.996"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConfusionMatrix(rede_neural_credit)\n",
        "cm.fit(x_credit_treinamento, y_credit_treinamento)\n",
        "cm.score(x_credit_teste, y_credit_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "glmpzuZwitFk",
        "outputId": "14d8f2d4-363a-45c5-88a6-753dd5b98940"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.996"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_credit_teste, previsao_credit))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ala4TyMiyCc",
        "outputId": "b79bb731-99c9-4183-dcda-23bc759324a0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       436\n",
            "           1       0.98      0.98      0.98        64\n",
            "\n",
            "    accuracy                           1.00       500\n",
            "   macro avg       0.99      0.99      0.99       500\n",
            "weighted avg       1.00      1.00      1.00       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(caminho + 'census.pkl', 'rb') as f:\n",
        "  x_census_treinamento, y_census_treinamento, x_census_teste, y_census_teste = pickle.load(f)"
      ],
      "metadata": {
        "id": "GFZTzUJDjE43"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_census_treinamento.shape, y_census_treinamento.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZoeN6AwjXRZ",
        "outputId": "b30f9895-9427-414b-ca74-61abd5402118"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((27676, 108), (27676,))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_census_teste.shape, y_census_teste.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUl_FJ8djboG",
        "outputId": "638283d5-3668-4abb-bd87-33eee5540b99"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4885, 108), (4885,))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rede_neural_census = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,\n",
        "hidden_layer_sizes=(55,55))\n",
        "rede_neural_census.fit(x_census_treinamento, y_census_treinamento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OCSkHd1MjhEI",
        "outputId": "32821c76-daee-47ad-c3f8-3af09de858be"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.39166120\n",
            "Iteration 2, loss = 0.32543042\n",
            "Iteration 3, loss = 0.31496176\n",
            "Iteration 4, loss = 0.30741972\n",
            "Iteration 5, loss = 0.30241458\n",
            "Iteration 6, loss = 0.29898194\n",
            "Iteration 7, loss = 0.29642928\n",
            "Iteration 8, loss = 0.29294387\n",
            "Iteration 9, loss = 0.29040342\n",
            "Iteration 10, loss = 0.28778983\n",
            "Iteration 11, loss = 0.28668221\n",
            "Iteration 12, loss = 0.28373420\n",
            "Iteration 13, loss = 0.28199503\n",
            "Iteration 14, loss = 0.28025453\n",
            "Iteration 15, loss = 0.27843193\n",
            "Iteration 16, loss = 0.27647261\n",
            "Iteration 17, loss = 0.27505147\n",
            "Iteration 18, loss = 0.27373950\n",
            "Iteration 19, loss = 0.27241475\n",
            "Iteration 20, loss = 0.27015524\n",
            "Iteration 21, loss = 0.26878374\n",
            "Iteration 22, loss = 0.26777468\n",
            "Iteration 23, loss = 0.26641893\n",
            "Iteration 24, loss = 0.26379565\n",
            "Iteration 25, loss = 0.26268311\n",
            "Iteration 26, loss = 0.26111174\n",
            "Iteration 27, loss = 0.26085959\n",
            "Iteration 28, loss = 0.25818087\n",
            "Iteration 29, loss = 0.25771391\n",
            "Iteration 30, loss = 0.25623443\n",
            "Iteration 31, loss = 0.25394121\n",
            "Iteration 32, loss = 0.25357037\n",
            "Iteration 33, loss = 0.25239941\n",
            "Iteration 34, loss = 0.25056586\n",
            "Iteration 35, loss = 0.24931441\n",
            "Iteration 36, loss = 0.24851498\n",
            "Iteration 37, loss = 0.24709952\n",
            "Iteration 38, loss = 0.24562425\n",
            "Iteration 39, loss = 0.24514895\n",
            "Iteration 40, loss = 0.24383057\n",
            "Iteration 41, loss = 0.24217649\n",
            "Iteration 42, loss = 0.24079447\n",
            "Iteration 43, loss = 0.23955329\n",
            "Iteration 44, loss = 0.24057859\n",
            "Iteration 45, loss = 0.23802720\n",
            "Iteration 46, loss = 0.23909498\n",
            "Iteration 47, loss = 0.23846035\n",
            "Iteration 48, loss = 0.23567791\n",
            "Iteration 49, loss = 0.23457052\n",
            "Iteration 50, loss = 0.23387809\n",
            "Iteration 51, loss = 0.23222659\n",
            "Iteration 52, loss = 0.23290220\n",
            "Iteration 53, loss = 0.23046248\n",
            "Iteration 54, loss = 0.22953496\n",
            "Iteration 55, loss = 0.22942213\n",
            "Iteration 56, loss = 0.22736448\n",
            "Iteration 57, loss = 0.22812767\n",
            "Iteration 58, loss = 0.22611747\n",
            "Iteration 59, loss = 0.22495417\n",
            "Iteration 60, loss = 0.22508531\n",
            "Iteration 61, loss = 0.22460029\n",
            "Iteration 62, loss = 0.22311174\n",
            "Iteration 63, loss = 0.22224983\n",
            "Iteration 64, loss = 0.22280874\n",
            "Iteration 65, loss = 0.22088045\n",
            "Iteration 66, loss = 0.22018509\n",
            "Iteration 67, loss = 0.21928457\n",
            "Iteration 68, loss = 0.21761261\n",
            "Iteration 69, loss = 0.21674374\n",
            "Iteration 70, loss = 0.21705119\n",
            "Iteration 71, loss = 0.21764943\n",
            "Iteration 72, loss = 0.21640234\n",
            "Iteration 73, loss = 0.21419869\n",
            "Iteration 74, loss = 0.21318595\n",
            "Iteration 75, loss = 0.21339070\n",
            "Iteration 76, loss = 0.21307354\n",
            "Iteration 77, loss = 0.21317458\n",
            "Iteration 78, loss = 0.21271477\n",
            "Iteration 79, loss = 0.21085854\n",
            "Iteration 80, loss = 0.21066409\n",
            "Iteration 81, loss = 0.21006812\n",
            "Iteration 82, loss = 0.21069167\n",
            "Iteration 83, loss = 0.20804725\n",
            "Iteration 84, loss = 0.20880005\n",
            "Iteration 85, loss = 0.20704553\n",
            "Iteration 86, loss = 0.20762528\n",
            "Iteration 87, loss = 0.20724708\n",
            "Iteration 88, loss = 0.20428335\n",
            "Iteration 89, loss = 0.20558764\n",
            "Iteration 90, loss = 0.20436407\n",
            "Iteration 91, loss = 0.20370593\n",
            "Iteration 92, loss = 0.20517372\n",
            "Iteration 93, loss = 0.20355927\n",
            "Iteration 94, loss = 0.20169499\n",
            "Iteration 95, loss = 0.20216354\n",
            "Iteration 96, loss = 0.20163577\n",
            "Iteration 97, loss = 0.20120317\n",
            "Iteration 98, loss = 0.19991699\n",
            "Iteration 99, loss = 0.19965171\n",
            "Iteration 100, loss = 0.19940740\n",
            "Iteration 101, loss = 0.19858473\n",
            "Iteration 102, loss = 0.19970494\n",
            "Iteration 103, loss = 0.19728811\n",
            "Iteration 104, loss = 0.19746441\n",
            "Iteration 105, loss = 0.19687943\n",
            "Iteration 106, loss = 0.19703480\n",
            "Iteration 107, loss = 0.19735529\n",
            "Iteration 108, loss = 0.19736195\n",
            "Iteration 109, loss = 0.19573796\n",
            "Iteration 110, loss = 0.19629544\n",
            "Iteration 111, loss = 0.19502708\n",
            "Iteration 112, loss = 0.19432344\n",
            "Iteration 113, loss = 0.19338914\n",
            "Iteration 114, loss = 0.19276774\n",
            "Iteration 115, loss = 0.19302096\n",
            "Iteration 116, loss = 0.19222034\n",
            "Iteration 117, loss = 0.19141066\n",
            "Iteration 118, loss = 0.19142996\n",
            "Iteration 119, loss = 0.19063581\n",
            "Iteration 120, loss = 0.19139133\n",
            "Iteration 121, loss = 0.19246558\n",
            "Iteration 122, loss = 0.19093901\n",
            "Iteration 123, loss = 0.19034373\n",
            "Iteration 124, loss = 0.18970836\n",
            "Iteration 125, loss = 0.18864459\n",
            "Iteration 126, loss = 0.18898425\n",
            "Iteration 127, loss = 0.18781743\n",
            "Iteration 128, loss = 0.18821668\n",
            "Iteration 129, loss = 0.18671367\n",
            "Iteration 130, loss = 0.18660219\n",
            "Iteration 131, loss = 0.18548651\n",
            "Iteration 132, loss = 0.18560711\n",
            "Iteration 133, loss = 0.18634926\n",
            "Iteration 134, loss = 0.18464006\n",
            "Iteration 135, loss = 0.18423075\n",
            "Iteration 136, loss = 0.18408327\n",
            "Iteration 137, loss = 0.18385721\n",
            "Iteration 138, loss = 0.18276557\n",
            "Iteration 139, loss = 0.18518041\n",
            "Iteration 140, loss = 0.18341893\n",
            "Iteration 141, loss = 0.18426416\n",
            "Iteration 142, loss = 0.18444798\n",
            "Iteration 143, loss = 0.18202901\n",
            "Iteration 144, loss = 0.18163534\n",
            "Iteration 145, loss = 0.18150606\n",
            "Iteration 146, loss = 0.18235578\n",
            "Iteration 147, loss = 0.18172371\n",
            "Iteration 148, loss = 0.17972061\n",
            "Iteration 149, loss = 0.18072283\n",
            "Iteration 150, loss = 0.18026054\n",
            "Iteration 151, loss = 0.17872040\n",
            "Iteration 152, loss = 0.17891637\n",
            "Iteration 153, loss = 0.17864640\n",
            "Iteration 154, loss = 0.17949909\n",
            "Iteration 155, loss = 0.17978570\n",
            "Iteration 156, loss = 0.17999839\n",
            "Iteration 157, loss = 0.17720359\n",
            "Iteration 158, loss = 0.17644764\n",
            "Iteration 159, loss = 0.17663576\n",
            "Iteration 160, loss = 0.17814033\n",
            "Iteration 161, loss = 0.17501357\n",
            "Iteration 162, loss = 0.17624473\n",
            "Iteration 163, loss = 0.17632842\n",
            "Iteration 164, loss = 0.17621922\n",
            "Iteration 165, loss = 0.17586529\n",
            "Iteration 166, loss = 0.17511050\n",
            "Iteration 167, loss = 0.17408380\n",
            "Iteration 168, loss = 0.17542153\n",
            "Iteration 169, loss = 0.17536097\n",
            "Iteration 170, loss = 0.17401589\n",
            "Iteration 171, loss = 0.17516845\n",
            "Iteration 172, loss = 0.17398848\n",
            "Iteration 173, loss = 0.17310120\n",
            "Iteration 174, loss = 0.17293269\n",
            "Iteration 175, loss = 0.17361609\n",
            "Iteration 176, loss = 0.17390054\n",
            "Iteration 177, loss = 0.17383298\n",
            "Iteration 178, loss = 0.17438367\n",
            "Iteration 179, loss = 0.17492019\n",
            "Iteration 180, loss = 0.17215055\n",
            "Iteration 181, loss = 0.17100277\n",
            "Iteration 182, loss = 0.17079825\n",
            "Iteration 183, loss = 0.17050751\n",
            "Iteration 184, loss = 0.17072467\n",
            "Iteration 185, loss = 0.17074761\n",
            "Iteration 186, loss = 0.17053717\n",
            "Iteration 187, loss = 0.16962342\n",
            "Iteration 188, loss = 0.16876236\n",
            "Iteration 189, loss = 0.16889730\n",
            "Iteration 190, loss = 0.16949364\n",
            "Iteration 191, loss = 0.16903925\n",
            "Iteration 192, loss = 0.16853559\n",
            "Iteration 193, loss = 0.16880852\n",
            "Iteration 194, loss = 0.16693125\n",
            "Iteration 195, loss = 0.16829730\n",
            "Iteration 196, loss = 0.16722905\n",
            "Iteration 197, loss = 0.16742751\n",
            "Iteration 198, loss = 0.16757051\n",
            "Iteration 199, loss = 0.16774170\n",
            "Iteration 200, loss = 0.16607003\n",
            "Iteration 201, loss = 0.16706308\n",
            "Iteration 202, loss = 0.16746462\n",
            "Iteration 203, loss = 0.16617878\n",
            "Iteration 204, loss = 0.16534635\n",
            "Iteration 205, loss = 0.16661470\n",
            "Iteration 206, loss = 0.16711357\n",
            "Iteration 207, loss = 0.16536863\n",
            "Iteration 208, loss = 0.16607258\n",
            "Iteration 209, loss = 0.16538410\n",
            "Iteration 210, loss = 0.16409621\n",
            "Iteration 211, loss = 0.16482211\n",
            "Iteration 212, loss = 0.16617514\n",
            "Iteration 213, loss = 0.16316976\n",
            "Iteration 214, loss = 0.16411745\n",
            "Iteration 215, loss = 0.16543180\n",
            "Iteration 216, loss = 0.16257499\n",
            "Iteration 217, loss = 0.16411116\n",
            "Iteration 218, loss = 0.16309441\n",
            "Iteration 219, loss = 0.16278978\n",
            "Iteration 220, loss = 0.16193608\n",
            "Iteration 221, loss = 0.16218864\n",
            "Iteration 222, loss = 0.16421261\n",
            "Iteration 223, loss = 0.16183316\n",
            "Iteration 224, loss = 0.16231400\n",
            "Iteration 225, loss = 0.16126467\n",
            "Iteration 226, loss = 0.16255644\n",
            "Iteration 227, loss = 0.16326026\n",
            "Iteration 228, loss = 0.16323969\n",
            "Iteration 229, loss = 0.16053722\n",
            "Iteration 230, loss = 0.15954051\n",
            "Iteration 231, loss = 0.15982899\n",
            "Iteration 232, loss = 0.15839433\n",
            "Iteration 233, loss = 0.16031365\n",
            "Iteration 234, loss = 0.15930324\n",
            "Iteration 235, loss = 0.16014988\n",
            "Iteration 236, loss = 0.16129338\n",
            "Iteration 237, loss = 0.16080418\n",
            "Iteration 238, loss = 0.15873852\n",
            "Iteration 239, loss = 0.16050676\n",
            "Iteration 240, loss = 0.15880817\n",
            "Iteration 241, loss = 0.15822873\n",
            "Iteration 242, loss = 0.15814207\n",
            "Iteration 243, loss = 0.15849868\n",
            "Iteration 244, loss = 0.15840016\n",
            "Iteration 245, loss = 0.15714778\n",
            "Iteration 246, loss = 0.15846111\n",
            "Iteration 247, loss = 0.15704727\n",
            "Iteration 248, loss = 0.15685634\n",
            "Iteration 249, loss = 0.15753294\n",
            "Iteration 250, loss = 0.15799471\n",
            "Iteration 251, loss = 0.15815347\n",
            "Iteration 252, loss = 0.15579671\n",
            "Iteration 253, loss = 0.15727483\n",
            "Iteration 254, loss = 0.15584006\n",
            "Iteration 255, loss = 0.15616668\n",
            "Iteration 256, loss = 0.15424832\n",
            "Iteration 257, loss = 0.15600156\n",
            "Iteration 258, loss = 0.15448570\n",
            "Iteration 259, loss = 0.15471336\n",
            "Iteration 260, loss = 0.15528821\n",
            "Iteration 261, loss = 0.15378443\n",
            "Iteration 262, loss = 0.15369094\n",
            "Iteration 263, loss = 0.15418978\n",
            "Iteration 264, loss = 0.15304593\n",
            "Iteration 265, loss = 0.15388323\n",
            "Iteration 266, loss = 0.15444551\n",
            "Iteration 267, loss = 0.15184080\n",
            "Iteration 268, loss = 0.15460494\n",
            "Iteration 269, loss = 0.15432919\n",
            "Iteration 270, loss = 0.15389200\n",
            "Iteration 271, loss = 0.15302713\n",
            "Iteration 272, loss = 0.15307108\n",
            "Iteration 273, loss = 0.15400205\n",
            "Iteration 274, loss = 0.15293245\n",
            "Iteration 275, loss = 0.15246126\n",
            "Iteration 276, loss = 0.15336539\n",
            "Iteration 277, loss = 0.15233232\n",
            "Iteration 278, loss = 0.15200234\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-4 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-4 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-4 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-4 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-4 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-4 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-4 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-4 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1500, tol=1e-05,\n",
              "              verbose=True)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsao_census = rede_neural_census.predict(x_census_teste)"
      ],
      "metadata": {
        "id": "zcLTVKjckesK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_census_teste, previsao_census)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xfiZbftknGC",
        "outputId": "8fd34d96-5d03-4400-e5a0-71c02d0ec986"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8227226202661208"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConfusionMatrix(rede_neural_census)\n",
        "cm.fit(x_census_treinamento, y_census_treinamento)\n",
        "cm.score(x_census_teste, y_census_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "Ntjdq-v3l5i3",
        "outputId": "dce18eec-507d-4b0a-d8f9-ca2f83f8248e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8227226202661208"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_census_teste, previsao_census))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CvQyMhTmOvZ",
        "outputId": "b29fc0df-e903-4948-fe46-2d3ae8a010e9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.88      0.89      0.88      3693\n",
            "        >50K       0.64      0.62      0.63      1192\n",
            "\n",
            "    accuracy                           0.82      4885\n",
            "   macro avg       0.76      0.75      0.76      4885\n",
            "weighted avg       0.82      0.82      0.82      4885\n",
            "\n"
          ]
        }
      ]
    }
  ]
}